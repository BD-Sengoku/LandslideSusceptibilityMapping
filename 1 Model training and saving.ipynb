{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f0b191e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code shows the required python packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "from lightgbm import LGBMClassifier\n",
    "import hyperopt\n",
    "from hyperopt import hp\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "183a17c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Specify the type of features to use: either all metrics or derived indicators\n",
    "DEMorAll = \"AllMetrics\"  # Options: DemDerivedIndicators OR AllMetrics\n",
    "\n",
    "# Load the dataset\n",
    "data01 = pd.read_csv('1844Points.csv', encoding='gbk')\n",
    "\n",
    "# Rename columns for better understanding\n",
    "data01.columns = ['FID', 'X', 'Y', 'Aspect', 'Elevation', 'Distance to lineament', 'Lineament density', \n",
    "                  'NDVI', 'Plan curvature', 'Profile curvature', 'Slope', 'Slope length', 'STI',\n",
    "                  'SPI', 'TPI', 'TWI', 'VRM', 'LULC', 'Habitat', 'GDP',\n",
    "                  'Distance from River', 'Distance from Road', 'target']\n",
    "\n",
    "# Drop unnecessary columns based on the selected feature type\n",
    "columns_to_drop = ['FID', 'X', 'Y'] if DEMorAll == 'AllMetrics' else ['FID', 'X', 'Y', \n",
    "                                                                     'LULC', 'Habitat', 'GDP', 'NDVI', 'Distance from Road']\n",
    "data01.drop(columns=columns_to_drop, inplace=True)  # Remove specified columns from the dataset\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "# Define output file and model paths\n",
    "filename = 'Output\\light_{}.txt'.format(DEMorAll)\n",
    "model_path = 'Output\\light_{}.model'.format(DEMorAll)\n",
    "\n",
    "max_evals = 10  # Number of evaluations for hyperparameter tuning\n",
    "seed = 4  # Random seed for reproducibility\n",
    "n_splits = 4  # Number of folds for K-Fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a237e9ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ac772c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the column names\n",
    "header = data01.columns.tolist()\n",
    "\n",
    "# Split the dataset into two dataframes based on the target value\n",
    "data_0 = data01.loc[data01['target'] == 0]  # Rows with target = 0\n",
    "data_1 = data01.loc[data01['target'] == 1]  # Rows with target = 1\n",
    "\n",
    "# Split target = 0 data into 80% training and 20% validation sets\n",
    "data_0_X = data_0.drop(columns=[\"target\"], axis=1)\n",
    "data_0_Y = data_0.target\n",
    "train_0_X, valid_0_X, train_0_y, valid_0_y = train_test_split(data_0_X, data_0_Y, test_size=0.2, random_state=seed)\n",
    "save_TrainDate_0 = pd.DataFrame(np.column_stack([train_0_X, train_0_y]), columns=header)\n",
    "save_ValidDate_0 = pd.DataFrame(np.column_stack([valid_0_X, valid_0_y]), columns=header)\n",
    "\n",
    "# Split target = 1 data into 80% training and 20% validation sets\n",
    "data_1_X = data_1.drop(columns=[\"target\"], axis=1)\n",
    "data_1_Y = data_1.target\n",
    "train_1_X, valid_1_X, train_1_y, valid_1_y = train_test_split(data_1_X, data_1_Y, test_size=0.2, random_state=seed)\n",
    "save_TrainDate_1 = pd.DataFrame(np.column_stack([train_1_X, train_1_y]), columns=header)\n",
    "save_ValidDate_1 = pd.DataFrame(np.column_stack([valid_1_X, valid_1_y]), columns=header)\n",
    "\n",
    "# Combine training datasets and shuffle the data to avoid ordering bias\n",
    "train_date = pd.concat([save_TrainDate_0, save_TrainDate_1])\n",
    "train_date = train_date.sample(frac=1, random_state=42)\n",
    "\n",
    "# Combine validation datasets and shuffle the data to avoid ordering bias\n",
    "valid_date = pd.concat([save_ValidDate_0, save_ValidDate_1])\n",
    "valid_date = valid_date.sample(frac=1, random_state=42)\n",
    "\n",
    "# Separate features (X) and target (y) from the training dataset\n",
    "train_y = train_date.target\n",
    "train_X = train_date.drop(columns=[ \"target\"], axis=1)\n",
    "\n",
    "# Separate features (X) and target (y) from the validation dataset\n",
    "valid_y = valid_date.target\n",
    "valid_X = valid_date.drop(columns=[\"target\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3aeee500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************                                                                                         \n",
      "[LightGBM] [Warning] feature_fraction is set=0.5587714406863481, colsample_bytree=0.9 will be ignored. Current value: feature_fraction=0.5587714406863481\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7057905978483938, subsample=0.8 will be ignored. Current value: bagging_fraction=0.7057905978483938\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4            \n",
      "[LightGBM] [Warning] early_stopping_round is set=200, early_stopping_rounds=200 will be ignored. Current value: early_stopping_round=200\n",
      "[100]\ttraining's auc: 0.873306\tvalid_1's auc: 0.830317                                                                 \n",
      "[100]\ttraining's auc: 0.867624\tvalid_1's auc: 0.847579                                                                 \n",
      "[100]\ttraining's auc: 0.864162\tvalid_1's auc: 0.862321                                                                 \n",
      "[100]\ttraining's auc: 0.859909\tvalid_1's auc: 0.878079                                                                 \n",
      "Current best 1-AUC score is: 0.14542590091066435, AUC score is: 0.8545740990893357                                     \n",
      "******************************                                                                                         \n",
      "[100]\ttraining's auc: 0.883771\tvalid_1's auc: 0.837232                                                                 \n",
      "[100]\ttraining's auc: 0.882502\tvalid_1's auc: 0.846873                                                                 \n",
      "[100]\ttraining's auc: 0.875401\tvalid_1's auc: 0.872556                                                                 \n",
      "[100]\ttraining's auc: 0.877689\tvalid_1's auc: 0.882511                                                                 \n",
      "Current best 1-AUC score is: 0.1402069230040072, AUC score is: 0.8597930769959928                                      \n",
      "******************************                                                                                         \n",
      "[100]\ttraining's auc: 0.881529\tvalid_1's auc: 0.83323                                                                  \n",
      "[100]\ttraining's auc: 0.87392\tvalid_1's auc: 0.852138                                                                  \n",
      "[100]\ttraining's auc: 0.873962\tvalid_1's auc: 0.871045                                                                 \n",
      "[100]\ttraining's auc: 0.870827\tvalid_1's auc: 0.877338                                                                 \n",
      "Current best 1-AUC score is: 0.142215782056439, AUC score is: 0.857784217943561                                        \n",
      "******************************                                                                                         \n",
      "[100]\ttraining's auc: 0.969875\tvalid_1's auc: 0.810764                                                                 \n",
      "[100]\ttraining's auc: 0.977992\tvalid_1's auc: 0.816666                                                                 \n",
      "[100]\ttraining's auc: 0.975657\tvalid_1's auc: 0.826638                                                                 \n",
      "[100]\ttraining's auc: 0.973381\tvalid_1's auc: 0.826323                                                                 \n",
      "Current best 1-AUC score is: 0.17974792602129538, AUC score is: 0.8202520739787046                                     \n",
      "******************************                                                                                         \n",
      "[100]\ttraining's auc: 0.96194\tvalid_1's auc: 0.839513                                                                  \n",
      "[100]\ttraining's auc: 0.96146\tvalid_1's auc: 0.855374                                                                  \n",
      "[100]\ttraining's auc: 0.959808\tvalid_1's auc: 0.861506                                                                 \n",
      "[100]\ttraining's auc: 0.954323\tvalid_1's auc: 0.869838                                                                 \n",
      "Current best 1-AUC score is: 0.14344222331985385, AUC score is: 0.8565577766801462                                     \n",
      "******************************                                                                                         \n",
      "[100]\ttraining's auc: 1\tvalid_1's auc: 0.805291                                                                        \n",
      "[100]\ttraining's auc: 0.999984\tvalid_1's auc: 0.822725                                                                 \n",
      "[100]\ttraining's auc: 1\tvalid_1's auc: 0.817484                                                                        \n",
      "[100]\ttraining's auc: 1\tvalid_1's auc: 0.786928                                                                        \n",
      "Current best 1-AUC score is: 0.19391935581110314, AUC score is: 0.8060806441888968                                     \n",
      "******************************                                                                                         \n",
      "[100]\ttraining's auc: 0.873499\tvalid_1's auc: 0.826624                                                                 \n",
      "[100]\ttraining's auc: 0.875559\tvalid_1's auc: 0.847697                                                                 \n",
      "[100]\ttraining's auc: 0.87101\tvalid_1's auc: 0.871786                                                                  \n",
      "[100]\ttraining's auc: 0.86698\tvalid_1's auc: 0.880984                                                                  \n",
      "Current best 1-AUC score is: 0.143227221290069, AUC score is: 0.856772778709931                                        \n",
      "******************************                                                                                         \n",
      "[100]\ttraining's auc: 0.845247\tvalid_1's auc: 0.811117                                                                 \n",
      "[100]\ttraining's auc: 0.841372\tvalid_1's auc: 0.820651                                                                 \n",
      "[100]\ttraining's auc: 0.832169\tvalid_1's auc: 0.847449                                                                 \n",
      "[100]\ttraining's auc: 0.82981\tvalid_1's auc: 0.859627                                                                  \n",
      "Current best 1-AUC score is: 0.1644233007249023, AUC score is: 0.8355766992750977                                      \n",
      "******************************                                                                                         \n",
      "[100]\ttraining's auc: 0.964427\tvalid_1's auc: 0.83148                                                                  \n",
      "[100]\ttraining's auc: 0.965516\tvalid_1's auc: 0.842697                                                                 \n",
      "[100]\ttraining's auc: 0.965506\tvalid_1's auc: 0.856292                                                                 \n",
      "[100]\ttraining's auc: 0.961693\tvalid_1's auc: 0.853653                                                                 \n",
      "Current best 1-AUC score is: 0.15396954016908213, AUC score is: 0.8460304598309178                                     \n",
      "******************************                                                                                         \n",
      "[100]\ttraining's auc: 0.854487\tvalid_1's auc: 0.810587                                                                 \n",
      "[100]\ttraining's auc: 0.840012\tvalid_1's auc: 0.819166                                                                 \n",
      "[100]\ttraining's auc: 0.832429\tvalid_1's auc: 0.848086                                                                 \n",
      "[100]\ttraining's auc: 0.831873\tvalid_1's auc: 0.85392                                                                  \n",
      "Current best 1-AUC score is: 0.1673603344924007, AUC score is: 0.8326396655075993                                      \n",
      "100%|████████████████████████████████████████████████| 10/10 [00:01<00:00,  5.81trial/s, best loss: 0.1402069230040072]\n",
      "Best parameters\n",
      "{'bagging_fraction': 0.725056679787422, 'bagging_freq': 1, 'boosting_type': 1, 'colsample_bytree': 1, 'feature_fraction': 0.7782240830826372, 'learning_rate': 0.41849913695193125, 'max_depth': 2, 'num_leaves': 97, 'reg_alpha': 0.011488008260217454, 'reg_lambda': 766.0218990930833, 'subsample': 0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define cross-validation function\n",
    "def cross_validation(model_params, train_X_fold, train_y_fold, valid_X_fold, valid_y_fold):\n",
    "    gbm = LGBMClassifier(**model_params)\n",
    "    gbm.fit(train_X_fold, train_y_fold, eval_set=[(train_X_fold, train_y_fold), \n",
    "                                                  (valid_X_fold, valid_y_fold)], verbose=100)\n",
    "    best_score = gbm.best_score_['valid_1']['auc']\n",
    "    return 1 - best_score  # Return 1 - AUC as hyperopt minimizes the objective\n",
    "\n",
    "# Define the hyperparameter optimization objective function\n",
    "def hyperopt_objective(params):\n",
    "    print(\"*\" * 30)\n",
    "    cur_param = {\n",
    "        'objective': 'binary',\n",
    "        'early_stopping_rounds': 200,\n",
    "        'metric': 'auc',\n",
    "        'importance_type': 'gain',\n",
    "        'max_depth': params['max_depth'],\n",
    "        'subsample': params['subsample'],\n",
    "        'colsample_bytree': params['colsample_bytree'],\n",
    "        'reg_alpha': params['reg_alpha'],\n",
    "        'reg_lambda': params['reg_lambda'],\n",
    "        'num_leaves': params['num_leaves'],\n",
    "        'learning_rate': params['learning_rate'],\n",
    "        'boosting_type': params['boosting_type'],\n",
    "        'bagging_freq': params['bagging_freq'],\n",
    "        'bagging_fraction': params['bagging_fraction'],\n",
    "        'feature_fraction': params['feature_fraction']\n",
    "    }\n",
    "    res = 0\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    for train_index, valid_index in kf.split(train_X):\n",
    "        train_X_fold, train_y_fold = train_X.iloc[train_index], train_y.iloc[train_index]\n",
    "        valid_X_fold, valid_y_fold = train_X.iloc[valid_index], train_y.iloc[valid_index]\n",
    "        res += cross_validation(cur_param, train_X_fold, train_y_fold, valid_X_fold, valid_y_fold)\n",
    "    res /= n_splits\n",
    "    print(\"Current best 1-AUC score is: {}, AUC score is: {}\".format(res, 1 - res))\n",
    "    return res  # Minimize objective\n",
    "\n",
    "# Define hyperparameter search space\n",
    "params_space = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'importance_type': 'gain',\n",
    "    'max_depth': hp.choice('max_depth', range(1, 5)),\n",
    "    'subsample': hp.choice('subsample', [0.8, 0.9, 1.0]),\n",
    "    'colsample_bytree': hp.choice('colsample_bytree', [0.8, 0.9, 1.0]),\n",
    "    'reg_alpha': hp.loguniform('reg_alpha', np.log(0.01), np.log(1000)),\n",
    "    'reg_lambda': hp.loguniform('reg_lambda', np.log(0.01), np.log(1000)),\n",
    "    'boosting_type': hp.choice('boosting_type', ['gbdt', 'dart', 'rf']),\n",
    "    'num_leaves': hp.choice('num_leaves', range(15, 128)),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.5),\n",
    "    'bagging_freq': hp.choice('bagging_freq', range(4, 7)),\n",
    "    'bagging_fraction': hp.uniform('bagging_fraction', 0.5, 0.9),\n",
    "    'feature_fraction': hp.uniform('feature_fraction', 0.5, 0.9)\n",
    "}\n",
    "\n",
    "# Initialize hyperopt trials\n",
    "trials = hyperopt.Trials()\n",
    "\n",
    "# Perform hyperparameter optimization\n",
    "best = hyperopt.fmin(\n",
    "    hyperopt_objective,\n",
    "    space=params_space,\n",
    "    algo=hyperopt.tpe.suggest,\n",
    "    max_evals=max_evals,\n",
    "    trials=trials)\n",
    "\n",
    "print(\"Best parameters\")\n",
    "print(best)\n",
    "\n",
    "# Extract the best parameters and add additional settings\n",
    "best_params = hyperopt.space_eval(params_space, best)\n",
    "best_params['objective'] = 'binary'\n",
    "best_params['metric'] = 'auc'\n",
    "best_params['num_iterations'] = 500\n",
    "best_params['early_stopping_rounds'] = 200\n",
    "best_params['importance_type'] = 'split'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cd74952e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's auc: 0.782396\tvalid_1's auc: 0.752885\n",
      "[2]\ttraining's auc: 0.788338\tvalid_1's auc: 0.74504\n",
      "[3]\ttraining's auc: 0.835214\tvalid_1's auc: 0.794492\n",
      "[4]\ttraining's auc: 0.835642\tvalid_1's auc: 0.794872\n",
      "[5]\ttraining's auc: 0.840854\tvalid_1's auc: 0.796713\n",
      "[6]\ttraining's auc: 0.839118\tvalid_1's auc: 0.793543\n",
      "[7]\ttraining's auc: 0.838559\tvalid_1's auc: 0.791775\n",
      "[8]\ttraining's auc: 0.838743\tvalid_1's auc: 0.792082\n",
      "[9]\ttraining's auc: 0.83812\tvalid_1's auc: 0.790855\n",
      "[10]\ttraining's auc: 0.837509\tvalid_1's auc: 0.790153\n",
      "[11]\ttraining's auc: 0.855254\tvalid_1's auc: 0.810679\n",
      "[12]\ttraining's auc: 0.854492\tvalid_1's auc: 0.810037\n",
      "[13]\ttraining's auc: 0.858127\tvalid_1's auc: 0.815486\n",
      "[14]\ttraining's auc: 0.859633\tvalid_1's auc: 0.812885\n",
      "[15]\ttraining's auc: 0.862384\tvalid_1's auc: 0.817648\n",
      "[16]\ttraining's auc: 0.86433\tvalid_1's auc: 0.820307\n",
      "[17]\ttraining's auc: 0.865078\tvalid_1's auc: 0.821037\n",
      "[18]\ttraining's auc: 0.865379\tvalid_1's auc: 0.821169\n",
      "[19]\ttraining's auc: 0.865145\tvalid_1's auc: 0.820993\n",
      "[20]\ttraining's auc: 0.865036\tvalid_1's auc: 0.821549\n",
      "[21]\ttraining's auc: 0.866487\tvalid_1's auc: 0.824368\n",
      "[22]\ttraining's auc: 0.86893\tvalid_1's auc: 0.827129\n",
      "[23]\ttraining's auc: 0.869898\tvalid_1's auc: 0.830241\n",
      "[24]\ttraining's auc: 0.870671\tvalid_1's auc: 0.831074\n",
      "[25]\ttraining's auc: 0.870525\tvalid_1's auc: 0.831015\n",
      "[26]\ttraining's auc: 0.870604\tvalid_1's auc: 0.830636\n",
      "[27]\ttraining's auc: 0.871049\tvalid_1's auc: 0.831308\n",
      "[28]\ttraining's auc: 0.871124\tvalid_1's auc: 0.830928\n",
      "[29]\ttraining's auc: 0.871931\tvalid_1's auc: 0.832856\n",
      "[30]\ttraining's auc: 0.871957\tvalid_1's auc: 0.832389\n",
      "[31]\ttraining's auc: 0.872117\tvalid_1's auc: 0.832652\n",
      "[32]\ttraining's auc: 0.871738\tvalid_1's auc: 0.83347\n",
      "[33]\ttraining's auc: 0.872352\tvalid_1's auc: 0.834638\n",
      "[34]\ttraining's auc: 0.873017\tvalid_1's auc: 0.835895\n",
      "[35]\ttraining's auc: 0.872826\tvalid_1's auc: 0.835544\n",
      "[36]\ttraining's auc: 0.872661\tvalid_1's auc: 0.834989\n",
      "[37]\ttraining's auc: 0.87304\tvalid_1's auc: 0.834302\n",
      "[38]\ttraining's auc: 0.872995\tvalid_1's auc: 0.832155\n",
      "[39]\ttraining's auc: 0.872622\tvalid_1's auc: 0.830197\n",
      "[40]\ttraining's auc: 0.872461\tvalid_1's auc: 0.829905\n",
      "[41]\ttraining's auc: 0.872719\tvalid_1's auc: 0.830592\n",
      "[42]\ttraining's auc: 0.872634\tvalid_1's auc: 0.830299\n",
      "[43]\ttraining's auc: 0.873315\tvalid_1's auc: 0.831337\n",
      "[44]\ttraining's auc: 0.874522\tvalid_1's auc: 0.832447\n",
      "[45]\ttraining's auc: 0.875383\tvalid_1's auc: 0.834142\n",
      "[46]\ttraining's auc: 0.875584\tvalid_1's auc: 0.834025\n",
      "[47]\ttraining's auc: 0.875631\tvalid_1's auc: 0.833762\n",
      "[48]\ttraining's auc: 0.875603\tvalid_1's auc: 0.832885\n",
      "[49]\ttraining's auc: 0.875546\tvalid_1's auc: 0.83198\n",
      "[50]\ttraining's auc: 0.875668\tvalid_1's auc: 0.832652\n",
      "[51]\ttraining's auc: 0.876346\tvalid_1's auc: 0.834054\n",
      "[52]\ttraining's auc: 0.876449\tvalid_1's auc: 0.834039\n",
      "[53]\ttraining's auc: 0.876515\tvalid_1's auc: 0.834127\n",
      "[54]\ttraining's auc: 0.876996\tvalid_1's auc: 0.835559\n",
      "[55]\ttraining's auc: 0.877244\tvalid_1's auc: 0.836099\n",
      "[56]\ttraining's auc: 0.87726\tvalid_1's auc: 0.836041\n",
      "[57]\ttraining's auc: 0.877319\tvalid_1's auc: 0.835252\n",
      "[58]\ttraining's auc: 0.877523\tvalid_1's auc: 0.83569\n",
      "[59]\ttraining's auc: 0.877501\tvalid_1's auc: 0.835603\n",
      "[60]\ttraining's auc: 0.877705\tvalid_1's auc: 0.835121\n",
      "[61]\ttraining's auc: 0.877742\tvalid_1's auc: 0.835091\n",
      "[62]\ttraining's auc: 0.877551\tvalid_1's auc: 0.835077\n",
      "[63]\ttraining's auc: 0.877459\tvalid_1's auc: 0.834726\n",
      "[64]\ttraining's auc: 0.877367\tvalid_1's auc: 0.834609\n",
      "[65]\ttraining's auc: 0.877323\tvalid_1's auc: 0.834434\n",
      "[66]\ttraining's auc: 0.877017\tvalid_1's auc: 0.834186\n",
      "[67]\ttraining's auc: 0.876835\tvalid_1's auc: 0.833835\n",
      "[68]\ttraining's auc: 0.876718\tvalid_1's auc: 0.833514\n",
      "[69]\ttraining's auc: 0.876644\tvalid_1's auc: 0.833338\n",
      "[70]\ttraining's auc: 0.876515\tvalid_1's auc: 0.83328\n",
      "[71]\ttraining's auc: 0.87667\tvalid_1's auc: 0.833455\n",
      "[72]\ttraining's auc: 0.877325\tvalid_1's auc: 0.834098\n",
      "[73]\ttraining's auc: 0.878048\tvalid_1's auc: 0.834332\n",
      "[74]\ttraining's auc: 0.877951\tvalid_1's auc: 0.834039\n",
      "[75]\ttraining's auc: 0.878344\tvalid_1's auc: 0.834478\n",
      "[76]\ttraining's auc: 0.878248\tvalid_1's auc: 0.834653\n",
      "[77]\ttraining's auc: 0.8782\tvalid_1's auc: 0.834682\n",
      "[78]\ttraining's auc: 0.878204\tvalid_1's auc: 0.834624\n",
      "[79]\ttraining's auc: 0.877982\tvalid_1's auc: 0.834682\n",
      "[80]\ttraining's auc: 0.878273\tvalid_1's auc: 0.834434\n",
      "[81]\ttraining's auc: 0.878393\tvalid_1's auc: 0.834434\n",
      "[82]\ttraining's auc: 0.878546\tvalid_1's auc: 0.834522\n",
      "[83]\ttraining's auc: 0.878671\tvalid_1's auc: 0.834492\n",
      "[84]\ttraining's auc: 0.878583\tvalid_1's auc: 0.834668\n",
      "[85]\ttraining's auc: 0.878557\tvalid_1's auc: 0.834638\n",
      "[86]\ttraining's auc: 0.878666\tvalid_1's auc: 0.834609\n",
      "[87]\ttraining's auc: 0.878924\tvalid_1's auc: 0.834726\n",
      "[88]\ttraining's auc: 0.878877\tvalid_1's auc: 0.834872\n",
      "[89]\ttraining's auc: 0.87889\tvalid_1's auc: 0.834346\n",
      "[90]\ttraining's auc: 0.879067\tvalid_1's auc: 0.8342\n",
      "[91]\ttraining's auc: 0.878958\tvalid_1's auc: 0.834259\n",
      "[92]\ttraining's auc: 0.879187\tvalid_1's auc: 0.834259\n",
      "[93]\ttraining's auc: 0.879281\tvalid_1's auc: 0.833996\n",
      "[94]\ttraining's auc: 0.879375\tvalid_1's auc: 0.834346\n",
      "[95]\ttraining's auc: 0.879405\tvalid_1's auc: 0.834346\n",
      "[96]\ttraining's auc: 0.879397\tvalid_1's auc: 0.834229\n",
      "[97]\ttraining's auc: 0.879927\tvalid_1's auc: 0.834901\n",
      "[98]\ttraining's auc: 0.879907\tvalid_1's auc: 0.834872\n",
      "[99]\ttraining's auc: 0.880192\tvalid_1's auc: 0.835749\n",
      "[100]\ttraining's auc: 0.880317\tvalid_1's auc: 0.834843\n",
      "[101]\ttraining's auc: 0.880328\tvalid_1's auc: 0.83496\n",
      "[102]\ttraining's auc: 0.880345\tvalid_1's auc: 0.834785\n",
      "[103]\ttraining's auc: 0.880301\tvalid_1's auc: 0.834755\n",
      "[104]\ttraining's auc: 0.880376\tvalid_1's auc: 0.834697\n",
      "[105]\ttraining's auc: 0.880336\tvalid_1's auc: 0.834843\n",
      "[106]\ttraining's auc: 0.880334\tvalid_1's auc: 0.834697\n",
      "[107]\ttraining's auc: 0.880288\tvalid_1's auc: 0.834755\n",
      "[108]\ttraining's auc: 0.880357\tvalid_1's auc: 0.834697\n",
      "[109]\ttraining's auc: 0.880489\tvalid_1's auc: 0.834711\n",
      "[110]\ttraining's auc: 0.880511\tvalid_1's auc: 0.834858\n",
      "[111]\ttraining's auc: 0.880636\tvalid_1's auc: 0.834653\n",
      "[112]\ttraining's auc: 0.880555\tvalid_1's auc: 0.834711\n",
      "[113]\ttraining's auc: 0.880575\tvalid_1's auc: 0.834565\n",
      "[114]\ttraining's auc: 0.880646\tvalid_1's auc: 0.834887\n",
      "[115]\ttraining's auc: 0.880611\tvalid_1's auc: 0.834478\n",
      "[116]\ttraining's auc: 0.880757\tvalid_1's auc: 0.834887\n",
      "[117]\ttraining's auc: 0.880811\tvalid_1's auc: 0.834858\n",
      "[118]\ttraining's auc: 0.880768\tvalid_1's auc: 0.834741\n",
      "[119]\ttraining's auc: 0.880761\tvalid_1's auc: 0.83477\n",
      "[120]\ttraining's auc: 0.880747\tvalid_1's auc: 0.834682\n",
      "[121]\ttraining's auc: 0.880666\tvalid_1's auc: 0.834828\n",
      "[122]\ttraining's auc: 0.880643\tvalid_1's auc: 0.834536\n",
      "[123]\ttraining's auc: 0.880841\tvalid_1's auc: 0.834551\n",
      "[124]\ttraining's auc: 0.880959\tvalid_1's auc: 0.834668\n",
      "[125]\ttraining's auc: 0.880966\tvalid_1's auc: 0.834931\n",
      "[126]\ttraining's auc: 0.881056\tvalid_1's auc: 0.834843\n",
      "[127]\ttraining's auc: 0.881117\tvalid_1's auc: 0.834346\n",
      "[128]\ttraining's auc: 0.881091\tvalid_1's auc: 0.834405\n",
      "[129]\ttraining's auc: 0.881173\tvalid_1's auc: 0.834171\n",
      "[130]\ttraining's auc: 0.881435\tvalid_1's auc: 0.834463\n",
      "[131]\ttraining's auc: 0.881398\tvalid_1's auc: 0.834522\n",
      "[132]\ttraining's auc: 0.881884\tvalid_1's auc: 0.83645\n",
      "[133]\ttraining's auc: 0.882295\tvalid_1's auc: 0.836888\n",
      "[134]\ttraining's auc: 0.88226\tvalid_1's auc: 0.837093\n",
      "[135]\ttraining's auc: 0.882353\tvalid_1's auc: 0.838919\n",
      "[136]\ttraining's auc: 0.882375\tvalid_1's auc: 0.839182\n",
      "[137]\ttraining's auc: 0.882347\tvalid_1's auc: 0.839123\n",
      "[138]\ttraining's auc: 0.882474\tvalid_1's auc: 0.841329\n",
      "[139]\ttraining's auc: 0.882629\tvalid_1's auc: 0.842761\n",
      "[140]\ttraining's auc: 0.882608\tvalid_1's auc: 0.842732\n",
      "[141]\ttraining's auc: 0.882598\tvalid_1's auc: 0.842761\n",
      "[142]\ttraining's auc: 0.882556\tvalid_1's auc: 0.842966\n",
      "[143]\ttraining's auc: 0.882598\tvalid_1's auc: 0.843375\n",
      "[144]\ttraining's auc: 0.882591\tvalid_1's auc: 0.843287\n",
      "[145]\ttraining's auc: 0.882644\tvalid_1's auc: 0.843316\n",
      "[146]\ttraining's auc: 0.882655\tvalid_1's auc: 0.843258\n",
      "[147]\ttraining's auc: 0.882536\tvalid_1's auc: 0.843083\n",
      "[148]\ttraining's auc: 0.882547\tvalid_1's auc: 0.843316\n",
      "[149]\ttraining's auc: 0.882508\tvalid_1's auc: 0.843024\n",
      "[150]\ttraining's auc: 0.882636\tvalid_1's auc: 0.842703\n",
      "[151]\ttraining's auc: 0.882651\tvalid_1's auc: 0.842644\n",
      "[152]\ttraining's auc: 0.882599\tvalid_1's auc: 0.842586\n",
      "[153]\ttraining's auc: 0.882509\tvalid_1's auc: 0.84206\n",
      "[154]\ttraining's auc: 0.882575\tvalid_1's auc: 0.842001\n",
      "[155]\ttraining's auc: 0.883459\tvalid_1's auc: 0.841286\n",
      "[156]\ttraining's auc: 0.883433\tvalid_1's auc: 0.841227\n",
      "[157]\ttraining's auc: 0.883394\tvalid_1's auc: 0.840906\n",
      "[158]\ttraining's auc: 0.883367\tvalid_1's auc: 0.840877\n",
      "[159]\ttraining's auc: 0.883337\tvalid_1's auc: 0.840789\n",
      "[160]\ttraining's auc: 0.883273\tvalid_1's auc: 0.840643\n",
      "[161]\ttraining's auc: 0.883488\tvalid_1's auc: 0.841256\n",
      "[162]\ttraining's auc: 0.883564\tvalid_1's auc: 0.840993\n",
      "[163]\ttraining's auc: 0.88445\tvalid_1's auc: 0.840511\n",
      "[164]\ttraining's auc: 0.88473\tvalid_1's auc: 0.840745\n",
      "[165]\ttraining's auc: 0.884743\tvalid_1's auc: 0.840979\n",
      "[166]\ttraining's auc: 0.885524\tvalid_1's auc: 0.840044\n",
      "[167]\ttraining's auc: 0.885851\tvalid_1's auc: 0.840102\n",
      "[168]\ttraining's auc: 0.886329\tvalid_1's auc: 0.839635\n",
      "[169]\ttraining's auc: 0.886751\tvalid_1's auc: 0.838846\n",
      "[170]\ttraining's auc: 0.88673\tvalid_1's auc: 0.839138\n",
      "[171]\ttraining's auc: 0.886878\tvalid_1's auc: 0.839576\n",
      "[172]\ttraining's auc: 0.886854\tvalid_1's auc: 0.839401\n",
      "[173]\ttraining's auc: 0.887052\tvalid_1's auc: 0.839167\n",
      "[174]\ttraining's auc: 0.8873\tvalid_1's auc: 0.838846\n",
      "[175]\ttraining's auc: 0.887471\tvalid_1's auc: 0.838992\n",
      "[176]\ttraining's auc: 0.887491\tvalid_1's auc: 0.83905\n",
      "[177]\ttraining's auc: 0.887445\tvalid_1's auc: 0.839109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[178]\ttraining's auc: 0.887443\tvalid_1's auc: 0.839109\n",
      "[179]\ttraining's auc: 0.887489\tvalid_1's auc: 0.838758\n",
      "[180]\ttraining's auc: 0.887483\tvalid_1's auc: 0.838641\n",
      "[181]\ttraining's auc: 0.887445\tvalid_1's auc: 0.838466\n",
      "[182]\ttraining's auc: 0.887888\tvalid_1's auc: 0.839357\n",
      "[183]\ttraining's auc: 0.887817\tvalid_1's auc: 0.839416\n",
      "[184]\ttraining's auc: 0.887719\tvalid_1's auc: 0.839153\n",
      "[185]\ttraining's auc: 0.887629\tvalid_1's auc: 0.839123\n",
      "[186]\ttraining's auc: 0.88769\tvalid_1's auc: 0.839328\n",
      "[187]\ttraining's auc: 0.887725\tvalid_1's auc: 0.839562\n",
      "[188]\ttraining's auc: 0.887779\tvalid_1's auc: 0.839474\n",
      "[189]\ttraining's auc: 0.887733\tvalid_1's auc: 0.839503\n",
      "[190]\ttraining's auc: 0.887685\tvalid_1's auc: 0.839386\n",
      "[191]\ttraining's auc: 0.887943\tvalid_1's auc: 0.839825\n",
      "[192]\ttraining's auc: 0.888284\tvalid_1's auc: 0.840146\n",
      "[193]\ttraining's auc: 0.888402\tvalid_1's auc: 0.840526\n",
      "[194]\ttraining's auc: 0.888358\tvalid_1's auc: 0.840497\n",
      "[195]\ttraining's auc: 0.888349\tvalid_1's auc: 0.840584\n",
      "[196]\ttraining's auc: 0.888618\tvalid_1's auc: 0.841286\n",
      "[197]\ttraining's auc: 0.888563\tvalid_1's auc: 0.841373\n",
      "[198]\ttraining's auc: 0.888556\tvalid_1's auc: 0.841461\n",
      "[199]\ttraining's auc: 0.888657\tvalid_1's auc: 0.841607\n",
      "[200]\ttraining's auc: 0.888707\tvalid_1's auc: 0.841549\n",
      "[201]\ttraining's auc: 0.888589\tvalid_1's auc: 0.841782\n",
      "[202]\ttraining's auc: 0.888637\tvalid_1's auc: 0.841402\n",
      "[203]\ttraining's auc: 0.888687\tvalid_1's auc: 0.841432\n",
      "[204]\ttraining's auc: 0.888659\tvalid_1's auc: 0.841344\n",
      "[205]\ttraining's auc: 0.888641\tvalid_1's auc: 0.841256\n",
      "[206]\ttraining's auc: 0.888755\tvalid_1's auc: 0.842162\n",
      "[207]\ttraining's auc: 0.88872\tvalid_1's auc: 0.842104\n",
      "[208]\ttraining's auc: 0.888664\tvalid_1's auc: 0.841286\n",
      "[209]\ttraining's auc: 0.888596\tvalid_1's auc: 0.841373\n",
      "[210]\ttraining's auc: 0.888567\tvalid_1's auc: 0.841169\n",
      "[211]\ttraining's auc: 0.888585\tvalid_1's auc: 0.841081\n",
      "[212]\ttraining's auc: 0.888639\tvalid_1's auc: 0.841198\n",
      "[213]\ttraining's auc: 0.888806\tvalid_1's auc: 0.841432\n",
      "[214]\ttraining's auc: 0.888826\tvalid_1's auc: 0.84187\n",
      "[215]\ttraining's auc: 0.888906\tvalid_1's auc: 0.842045\n",
      "[216]\ttraining's auc: 0.888906\tvalid_1's auc: 0.842162\n",
      "[217]\ttraining's auc: 0.888865\tvalid_1's auc: 0.841987\n",
      "[218]\ttraining's auc: 0.888946\tvalid_1's auc: 0.842484\n",
      "[219]\ttraining's auc: 0.888903\tvalid_1's auc: 0.842337\n",
      "[220]\ttraining's auc: 0.888958\tvalid_1's auc: 0.842951\n",
      "[221]\ttraining's auc: 0.888925\tvalid_1's auc: 0.842922\n",
      "[222]\ttraining's auc: 0.889186\tvalid_1's auc: 0.843419\n",
      "[223]\ttraining's auc: 0.889365\tvalid_1's auc: 0.843828\n",
      "[224]\ttraining's auc: 0.889339\tvalid_1's auc: 0.84374\n",
      "[225]\ttraining's auc: 0.889286\tvalid_1's auc: 0.843682\n",
      "[226]\ttraining's auc: 0.88929\tvalid_1's auc: 0.843682\n",
      "[227]\ttraining's auc: 0.889278\tvalid_1's auc: 0.843711\n",
      "[228]\ttraining's auc: 0.889139\tvalid_1's auc: 0.843711\n",
      "[229]\ttraining's auc: 0.88908\tvalid_1's auc: 0.843828\n",
      "[230]\ttraining's auc: 0.889326\tvalid_1's auc: 0.844266\n",
      "[231]\ttraining's auc: 0.889658\tvalid_1's auc: 0.845581\n",
      "[232]\ttraining's auc: 0.889835\tvalid_1's auc: 0.846954\n",
      "[233]\ttraining's auc: 0.889861\tvalid_1's auc: 0.847012\n",
      "[234]\ttraining's auc: 0.889929\tvalid_1's auc: 0.847801\n",
      "[235]\ttraining's auc: 0.889933\tvalid_1's auc: 0.847772\n",
      "[236]\ttraining's auc: 0.889933\tvalid_1's auc: 0.847831\n",
      "[237]\ttraining's auc: 0.890421\tvalid_1's auc: 0.847977\n",
      "[238]\ttraining's auc: 0.890568\tvalid_1's auc: 0.847918\n",
      "[239]\ttraining's auc: 0.890982\tvalid_1's auc: 0.847626\n",
      "[240]\ttraining's auc: 0.891294\tvalid_1's auc: 0.84821\n",
      "[241]\ttraining's auc: 0.89127\tvalid_1's auc: 0.84821\n",
      "[242]\ttraining's auc: 0.891376\tvalid_1's auc: 0.848386\n",
      "[243]\ttraining's auc: 0.891303\tvalid_1's auc: 0.848356\n",
      "[244]\ttraining's auc: 0.891308\tvalid_1's auc: 0.848386\n",
      "[245]\ttraining's auc: 0.891303\tvalid_1's auc: 0.848035\n",
      "[246]\ttraining's auc: 0.891452\tvalid_1's auc: 0.848181\n",
      "[247]\ttraining's auc: 0.891654\tvalid_1's auc: 0.848181\n",
      "[248]\ttraining's auc: 0.891636\tvalid_1's auc: 0.84821\n",
      "[249]\ttraining's auc: 0.891625\tvalid_1's auc: 0.848123\n",
      "[250]\ttraining's auc: 0.891687\tvalid_1's auc: 0.848853\n",
      "[251]\ttraining's auc: 0.891654\tvalid_1's auc: 0.848853\n",
      "[252]\ttraining's auc: 0.891623\tvalid_1's auc: 0.848941\n",
      "[253]\ttraining's auc: 0.891699\tvalid_1's auc: 0.84897\n",
      "[254]\ttraining's auc: 0.891778\tvalid_1's auc: 0.848853\n",
      "[255]\ttraining's auc: 0.891859\tvalid_1's auc: 0.849408\n",
      "[256]\ttraining's auc: 0.891899\tvalid_1's auc: 0.849145\n",
      "[257]\ttraining's auc: 0.892089\tvalid_1's auc: 0.84897\n",
      "[258]\ttraining's auc: 0.892302\tvalid_1's auc: 0.848619\n",
      "[259]\ttraining's auc: 0.892267\tvalid_1's auc: 0.848649\n",
      "[260]\ttraining's auc: 0.892271\tvalid_1's auc: 0.848678\n",
      "[261]\ttraining's auc: 0.892194\tvalid_1's auc: 0.848736\n",
      "[262]\ttraining's auc: 0.892501\tvalid_1's auc: 0.849145\n",
      "[263]\ttraining's auc: 0.892698\tvalid_1's auc: 0.849175\n",
      "[264]\ttraining's auc: 0.892847\tvalid_1's auc: 0.849145\n",
      "[265]\ttraining's auc: 0.893033\tvalid_1's auc: 0.849379\n",
      "[266]\ttraining's auc: 0.893098\tvalid_1's auc: 0.849847\n",
      "[267]\ttraining's auc: 0.893076\tvalid_1's auc: 0.849876\n",
      "[268]\ttraining's auc: 0.893418\tvalid_1's auc: 0.848824\n",
      "[269]\ttraining's auc: 0.893394\tvalid_1's auc: 0.848882\n",
      "[270]\ttraining's auc: 0.893499\tvalid_1's auc: 0.848999\n",
      "[271]\ttraining's auc: 0.893494\tvalid_1's auc: 0.849028\n",
      "[272]\ttraining's auc: 0.893589\tvalid_1's auc: 0.849554\n",
      "[273]\ttraining's auc: 0.893667\tvalid_1's auc: 0.850022\n",
      "[274]\ttraining's auc: 0.893659\tvalid_1's auc: 0.850022\n",
      "[275]\ttraining's auc: 0.893775\tvalid_1's auc: 0.850314\n",
      "[276]\ttraining's auc: 0.893713\tvalid_1's auc: 0.850256\n",
      "[277]\ttraining's auc: 0.893744\tvalid_1's auc: 0.850168\n",
      "[278]\ttraining's auc: 0.893737\tvalid_1's auc: 0.850051\n",
      "[279]\ttraining's auc: 0.893716\tvalid_1's auc: 0.850256\n",
      "[280]\ttraining's auc: 0.893709\tvalid_1's auc: 0.850373\n",
      "[281]\ttraining's auc: 0.893705\tvalid_1's auc: 0.850226\n",
      "[282]\ttraining's auc: 0.893823\tvalid_1's auc: 0.850343\n",
      "[283]\ttraining's auc: 0.893792\tvalid_1's auc: 0.850373\n",
      "[284]\ttraining's auc: 0.893781\tvalid_1's auc: 0.85008\n",
      "[285]\ttraining's auc: 0.893795\tvalid_1's auc: 0.849788\n",
      "[286]\ttraining's auc: 0.894064\tvalid_1's auc: 0.849467\n",
      "[287]\ttraining's auc: 0.894175\tvalid_1's auc: 0.849175\n",
      "[288]\ttraining's auc: 0.894186\tvalid_1's auc: 0.849028\n",
      "[289]\ttraining's auc: 0.894431\tvalid_1's auc: 0.848941\n",
      "[290]\ttraining's auc: 0.894679\tvalid_1's auc: 0.848532\n",
      "[291]\ttraining's auc: 0.894685\tvalid_1's auc: 0.848415\n",
      "[292]\ttraining's auc: 0.894653\tvalid_1's auc: 0.848415\n",
      "[293]\ttraining's auc: 0.894631\tvalid_1's auc: 0.848415\n",
      "[294]\ttraining's auc: 0.894944\tvalid_1's auc: 0.848444\n",
      "[295]\ttraining's auc: 0.894915\tvalid_1's auc: 0.848444\n",
      "[296]\ttraining's auc: 0.894874\tvalid_1's auc: 0.848444\n",
      "[297]\ttraining's auc: 0.89509\tvalid_1's auc: 0.849379\n",
      "[298]\ttraining's auc: 0.895055\tvalid_1's auc: 0.84935\n",
      "[299]\ttraining's auc: 0.895046\tvalid_1's auc: 0.850343\n",
      "[300]\ttraining's auc: 0.895035\tvalid_1's auc: 0.85046\n",
      "[301]\ttraining's auc: 0.894996\tvalid_1's auc: 0.850431\n",
      "[302]\ttraining's auc: 0.895219\tvalid_1's auc: 0.849905\n",
      "[303]\ttraining's auc: 0.895182\tvalid_1's auc: 0.849817\n",
      "[304]\ttraining's auc: 0.89515\tvalid_1's auc: 0.849876\n",
      "[305]\ttraining's auc: 0.895106\tvalid_1's auc: 0.849963\n",
      "[306]\ttraining's auc: 0.89511\tvalid_1's auc: 0.850051\n",
      "[307]\ttraining's auc: 0.895088\tvalid_1's auc: 0.849993\n",
      "[308]\ttraining's auc: 0.895071\tvalid_1's auc: 0.849963\n",
      "[309]\ttraining's auc: 0.895053\tvalid_1's auc: 0.849788\n",
      "[310]\ttraining's auc: 0.895099\tvalid_1's auc: 0.85008\n",
      "[311]\ttraining's auc: 0.895222\tvalid_1's auc: 0.850168\n",
      "[312]\ttraining's auc: 0.89557\tvalid_1's auc: 0.849963\n",
      "[313]\ttraining's auc: 0.895635\tvalid_1's auc: 0.850022\n",
      "[314]\ttraining's auc: 0.895644\tvalid_1's auc: 0.849993\n",
      "[315]\ttraining's auc: 0.895581\tvalid_1's auc: 0.849876\n",
      "[316]\ttraining's auc: 0.895753\tvalid_1's auc: 0.850373\n",
      "[317]\ttraining's auc: 0.896036\tvalid_1's auc: 0.850256\n",
      "[318]\ttraining's auc: 0.896158\tvalid_1's auc: 0.85011\n",
      "[319]\ttraining's auc: 0.896124\tvalid_1's auc: 0.85011\n",
      "[320]\ttraining's auc: 0.896147\tvalid_1's auc: 0.850139\n",
      "[321]\ttraining's auc: 0.89611\tvalid_1's auc: 0.85008\n",
      "[322]\ttraining's auc: 0.896042\tvalid_1's auc: 0.849876\n",
      "[323]\ttraining's auc: 0.895959\tvalid_1's auc: 0.849876\n",
      "[324]\ttraining's auc: 0.895937\tvalid_1's auc: 0.849934\n",
      "[325]\ttraining's auc: 0.895924\tvalid_1's auc: 0.849817\n",
      "[326]\ttraining's auc: 0.89588\tvalid_1's auc: 0.849671\n",
      "[327]\ttraining's auc: 0.896071\tvalid_1's auc: 0.85008\n",
      "[328]\ttraining's auc: 0.896049\tvalid_1's auc: 0.850051\n",
      "[329]\ttraining's auc: 0.896286\tvalid_1's auc: 0.850256\n",
      "[330]\ttraining's auc: 0.896259\tvalid_1's auc: 0.850226\n",
      "[331]\ttraining's auc: 0.896222\tvalid_1's auc: 0.850285\n",
      "[332]\ttraining's auc: 0.896211\tvalid_1's auc: 0.850314\n",
      "[333]\ttraining's auc: 0.896207\tvalid_1's auc: 0.850314\n",
      "[334]\ttraining's auc: 0.896185\tvalid_1's auc: 0.850343\n",
      "[335]\ttraining's auc: 0.89631\tvalid_1's auc: 0.850285\n",
      "[336]\ttraining's auc: 0.896303\tvalid_1's auc: 0.850226\n",
      "[337]\ttraining's auc: 0.896382\tvalid_1's auc: 0.850197\n",
      "[338]\ttraining's auc: 0.896485\tvalid_1's auc: 0.851015\n",
      "[339]\ttraining's auc: 0.896491\tvalid_1's auc: 0.850928\n",
      "[340]\ttraining's auc: 0.89645\tvalid_1's auc: 0.850869\n",
      "[341]\ttraining's auc: 0.896546\tvalid_1's auc: 0.850986\n",
      "[342]\ttraining's auc: 0.896669\tvalid_1's auc: 0.851132\n",
      "[343]\ttraining's auc: 0.896752\tvalid_1's auc: 0.851278\n",
      "[344]\ttraining's auc: 0.896721\tvalid_1's auc: 0.85122\n",
      "[345]\ttraining's auc: 0.896758\tvalid_1's auc: 0.850957\n",
      "[346]\ttraining's auc: 0.8968\tvalid_1's auc: 0.85122\n",
      "[347]\ttraining's auc: 0.896809\tvalid_1's auc: 0.851191\n",
      "[348]\ttraining's auc: 0.896901\tvalid_1's auc: 0.851103\n",
      "[349]\ttraining's auc: 0.897025\tvalid_1's auc: 0.851015\n",
      "[350]\ttraining's auc: 0.896964\tvalid_1's auc: 0.850986\n",
      "[351]\ttraining's auc: 0.89705\tvalid_1's auc: 0.850986\n",
      "[352]\ttraining's auc: 0.897084\tvalid_1's auc: 0.851191\n",
      "[353]\ttraining's auc: 0.897113\tvalid_1's auc: 0.851191\n",
      "[354]\ttraining's auc: 0.897056\tvalid_1's auc: 0.851191\n",
      "[355]\ttraining's auc: 0.897008\tvalid_1's auc: 0.851074\n",
      "[356]\ttraining's auc: 0.897288\tvalid_1's auc: 0.8516\n",
      "[357]\ttraining's auc: 0.89729\tvalid_1's auc: 0.8516\n",
      "[358]\ttraining's auc: 0.89743\tvalid_1's auc: 0.851804\n",
      "[359]\ttraining's auc: 0.897413\tvalid_1's auc: 0.851863\n",
      "[360]\ttraining's auc: 0.897586\tvalid_1's auc: 0.852126\n",
      "[361]\ttraining's auc: 0.897774\tvalid_1's auc: 0.852944\n",
      "[362]\ttraining's auc: 0.897951\tvalid_1's auc: 0.852681\n",
      "[363]\ttraining's auc: 0.89793\tvalid_1's auc: 0.852535\n",
      "[364]\ttraining's auc: 0.89791\tvalid_1's auc: 0.852535\n",
      "[365]\ttraining's auc: 0.898091\tvalid_1's auc: 0.852155\n",
      "[366]\ttraining's auc: 0.898054\tvalid_1's auc: 0.852096\n",
      "[367]\ttraining's auc: 0.898037\tvalid_1's auc: 0.852038\n",
      "[368]\ttraining's auc: 0.898348\tvalid_1's auc: 0.852389\n",
      "[369]\ttraining's auc: 0.898289\tvalid_1's auc: 0.852359\n",
      "[370]\ttraining's auc: 0.898579\tvalid_1's auc: 0.852652\n",
      "[371]\ttraining's auc: 0.898529\tvalid_1's auc: 0.852593\n",
      "[372]\ttraining's auc: 0.898704\tvalid_1's auc: 0.852505\n",
      "[373]\ttraining's auc: 0.898711\tvalid_1's auc: 0.852009\n",
      "[374]\ttraining's auc: 0.898829\tvalid_1's auc: 0.852096\n",
      "[375]\ttraining's auc: 0.898879\tvalid_1's auc: 0.851687\n",
      "[376]\ttraining's auc: 0.898855\tvalid_1's auc: 0.851717\n",
      "[377]\ttraining's auc: 0.898912\tvalid_1's auc: 0.851483\n",
      "[378]\ttraining's auc: 0.898875\tvalid_1's auc: 0.851512\n",
      "[379]\ttraining's auc: 0.898829\tvalid_1's auc: 0.851454\n",
      "[380]\ttraining's auc: 0.898801\tvalid_1's auc: 0.851541\n",
      "[381]\ttraining's auc: 0.898776\tvalid_1's auc: 0.851541\n",
      "[382]\ttraining's auc: 0.898728\tvalid_1's auc: 0.851483\n",
      "[383]\ttraining's auc: 0.898753\tvalid_1's auc: 0.851337\n",
      "[384]\ttraining's auc: 0.898892\tvalid_1's auc: 0.851337\n",
      "[385]\ttraining's auc: 0.898862\tvalid_1's auc: 0.851278\n",
      "[386]\ttraining's auc: 0.898779\tvalid_1's auc: 0.851074\n",
      "[387]\ttraining's auc: 0.898753\tvalid_1's auc: 0.851161\n",
      "[388]\ttraining's auc: 0.898886\tvalid_1's auc: 0.851395\n",
      "[389]\ttraining's auc: 0.898812\tvalid_1's auc: 0.851249\n",
      "[390]\ttraining's auc: 0.899009\tvalid_1's auc: 0.852184\n",
      "[391]\ttraining's auc: 0.898969\tvalid_1's auc: 0.852243\n",
      "[392]\ttraining's auc: 0.899355\tvalid_1's auc: 0.852067\n",
      "[393]\ttraining's auc: 0.899424\tvalid_1's auc: 0.852038\n",
      "[394]\ttraining's auc: 0.899777\tvalid_1's auc: 0.852067\n",
      "[395]\ttraining's auc: 0.899799\tvalid_1's auc: 0.851863\n",
      "[396]\ttraining's auc: 0.899772\tvalid_1's auc: 0.851921\n",
      "[397]\ttraining's auc: 0.900026\tvalid_1's auc: 0.852389\n",
      "[398]\ttraining's auc: 0.900202\tvalid_1's auc: 0.852418\n",
      "[399]\ttraining's auc: 0.900438\tvalid_1's auc: 0.852243\n",
      "[400]\ttraining's auc: 0.900604\tvalid_1's auc: 0.852447\n",
      "[401]\ttraining's auc: 0.900587\tvalid_1's auc: 0.852418\n",
      "[402]\ttraining's auc: 0.900961\tvalid_1's auc: 0.85198\n",
      "[403]\ttraining's auc: 0.900933\tvalid_1's auc: 0.852009\n",
      "[404]\ttraining's auc: 0.900972\tvalid_1's auc: 0.852418\n",
      "[405]\ttraining's auc: 0.901143\tvalid_1's auc: 0.852272\n",
      "[406]\ttraining's auc: 0.901136\tvalid_1's auc: 0.852301\n",
      "[407]\ttraining's auc: 0.901099\tvalid_1's auc: 0.852213\n",
      "[408]\ttraining's auc: 0.901053\tvalid_1's auc: 0.852096\n",
      "[409]\ttraining's auc: 0.901075\tvalid_1's auc: 0.852067\n",
      "[410]\ttraining's auc: 0.901125\tvalid_1's auc: 0.852184\n",
      "[411]\ttraining's auc: 0.901283\tvalid_1's auc: 0.852096\n",
      "[412]\ttraining's auc: 0.901239\tvalid_1's auc: 0.852067\n",
      "[413]\ttraining's auc: 0.9012\tvalid_1's auc: 0.852067\n",
      "[414]\ttraining's auc: 0.901346\tvalid_1's auc: 0.851804\n",
      "[415]\ttraining's auc: 0.901331\tvalid_1's auc: 0.851863\n",
      "[416]\ttraining's auc: 0.901414\tvalid_1's auc: 0.85195\n",
      "[417]\ttraining's auc: 0.901379\tvalid_1's auc: 0.85198\n",
      "[418]\ttraining's auc: 0.901322\tvalid_1's auc: 0.85198\n",
      "[419]\ttraining's auc: 0.901368\tvalid_1's auc: 0.852213\n",
      "[420]\ttraining's auc: 0.901327\tvalid_1's auc: 0.852213\n",
      "[421]\ttraining's auc: 0.901491\tvalid_1's auc: 0.851921\n",
      "[422]\ttraining's auc: 0.901579\tvalid_1's auc: 0.85195\n",
      "[423]\ttraining's auc: 0.901806\tvalid_1's auc: 0.851366\n",
      "[424]\ttraining's auc: 0.902056\tvalid_1's auc: 0.8516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[425]\ttraining's auc: 0.902019\tvalid_1's auc: 0.851512\n",
      "[426]\ttraining's auc: 0.902069\tvalid_1's auc: 0.8516\n",
      "[427]\ttraining's auc: 0.902406\tvalid_1's auc: 0.851395\n",
      "[428]\ttraining's auc: 0.902351\tvalid_1's auc: 0.851395\n",
      "[429]\ttraining's auc: 0.902487\tvalid_1's auc: 0.851512\n",
      "[430]\ttraining's auc: 0.902461\tvalid_1's auc: 0.851454\n",
      "[431]\ttraining's auc: 0.90245\tvalid_1's auc: 0.851454\n",
      "[432]\ttraining's auc: 0.902572\tvalid_1's auc: 0.851308\n",
      "[433]\ttraining's auc: 0.902555\tvalid_1's auc: 0.851191\n",
      "[434]\ttraining's auc: 0.902533\tvalid_1's auc: 0.85122\n",
      "[435]\ttraining's auc: 0.902483\tvalid_1's auc: 0.851161\n",
      "[436]\ttraining's auc: 0.902699\tvalid_1's auc: 0.851278\n",
      "[437]\ttraining's auc: 0.902715\tvalid_1's auc: 0.850957\n",
      "[438]\ttraining's auc: 0.902701\tvalid_1's auc: 0.850869\n",
      "[439]\ttraining's auc: 0.90268\tvalid_1's auc: 0.85084\n",
      "[440]\ttraining's auc: 0.902765\tvalid_1's auc: 0.850957\n",
      "[441]\ttraining's auc: 0.903023\tvalid_1's auc: 0.850489\n",
      "[442]\ttraining's auc: 0.903078\tvalid_1's auc: 0.850548\n",
      "[443]\ttraining's auc: 0.903039\tvalid_1's auc: 0.850548\n",
      "[444]\ttraining's auc: 0.903142\tvalid_1's auc: 0.850694\n",
      "[445]\ttraining's auc: 0.903397\tvalid_1's auc: 0.850314\n",
      "[446]\ttraining's auc: 0.903349\tvalid_1's auc: 0.850314\n",
      "[447]\ttraining's auc: 0.903432\tvalid_1's auc: 0.850343\n",
      "[448]\ttraining's auc: 0.903527\tvalid_1's auc: 0.850577\n",
      "[449]\ttraining's auc: 0.903665\tvalid_1's auc: 0.850139\n",
      "[450]\ttraining's auc: 0.903805\tvalid_1's auc: 0.850226\n",
      "[451]\ttraining's auc: 0.90403\tvalid_1's auc: 0.850402\n",
      "[452]\ttraining's auc: 0.903973\tvalid_1's auc: 0.850343\n",
      "[453]\ttraining's auc: 0.903956\tvalid_1's auc: 0.850373\n",
      "[454]\ttraining's auc: 0.903932\tvalid_1's auc: 0.850343\n",
      "[455]\ttraining's auc: 0.904161\tvalid_1's auc: 0.850314\n",
      "[456]\ttraining's auc: 0.904135\tvalid_1's auc: 0.850314\n",
      "[457]\ttraining's auc: 0.904089\tvalid_1's auc: 0.850256\n",
      "[458]\ttraining's auc: 0.904175\tvalid_1's auc: 0.850577\n",
      "[459]\ttraining's auc: 0.90433\tvalid_1's auc: 0.850577\n",
      "[460]\ttraining's auc: 0.904358\tvalid_1's auc: 0.850519\n",
      "[461]\ttraining's auc: 0.90444\tvalid_1's auc: 0.850928\n",
      "[462]\ttraining's auc: 0.904407\tvalid_1's auc: 0.850986\n",
      "[463]\ttraining's auc: 0.904394\tvalid_1's auc: 0.850986\n",
      "[464]\ttraining's auc: 0.904444\tvalid_1's auc: 0.851424\n",
      "[465]\ttraining's auc: 0.904531\tvalid_1's auc: 0.852096\n",
      "[466]\ttraining's auc: 0.904496\tvalid_1's auc: 0.852096\n",
      "[467]\ttraining's auc: 0.904628\tvalid_1's auc: 0.852243\n",
      "[468]\ttraining's auc: 0.904612\tvalid_1's auc: 0.852272\n",
      "[469]\ttraining's auc: 0.904678\tvalid_1's auc: 0.852272\n",
      "[470]\ttraining's auc: 0.904637\tvalid_1's auc: 0.852243\n",
      "[471]\ttraining's auc: 0.904731\tvalid_1's auc: 0.852359\n",
      "[472]\ttraining's auc: 0.904718\tvalid_1's auc: 0.852272\n",
      "[473]\ttraining's auc: 0.904847\tvalid_1's auc: 0.852184\n",
      "[474]\ttraining's auc: 0.905026\tvalid_1's auc: 0.852155\n",
      "[475]\ttraining's auc: 0.905256\tvalid_1's auc: 0.851483\n",
      "[476]\ttraining's auc: 0.905274\tvalid_1's auc: 0.851454\n",
      "[477]\ttraining's auc: 0.905223\tvalid_1's auc: 0.851015\n",
      "[478]\ttraining's auc: 0.905184\tvalid_1's auc: 0.851191\n",
      "[479]\ttraining's auc: 0.905157\tvalid_1's auc: 0.851191\n",
      "[480]\ttraining's auc: 0.905289\tvalid_1's auc: 0.851366\n",
      "[481]\ttraining's auc: 0.905256\tvalid_1's auc: 0.851337\n",
      "[482]\ttraining's auc: 0.905435\tvalid_1's auc: 0.851161\n",
      "[483]\ttraining's auc: 0.905676\tvalid_1's auc: 0.851249\n",
      "[484]\ttraining's auc: 0.905685\tvalid_1's auc: 0.851074\n",
      "[485]\ttraining's auc: 0.905646\tvalid_1's auc: 0.851045\n",
      "[486]\ttraining's auc: 0.905867\tvalid_1's auc: 0.851337\n",
      "[487]\ttraining's auc: 0.906044\tvalid_1's auc: 0.85122\n",
      "[488]\ttraining's auc: 0.90623\tvalid_1's auc: 0.851191\n",
      "[489]\ttraining's auc: 0.906434\tvalid_1's auc: 0.851074\n",
      "[490]\ttraining's auc: 0.906397\tvalid_1's auc: 0.851103\n",
      "[491]\ttraining's auc: 0.906396\tvalid_1's auc: 0.851103\n",
      "[492]\ttraining's auc: 0.906386\tvalid_1's auc: 0.851103\n",
      "[493]\ttraining's auc: 0.906353\tvalid_1's auc: 0.851132\n",
      "[494]\ttraining's auc: 0.906434\tvalid_1's auc: 0.851366\n",
      "[495]\ttraining's auc: 0.906488\tvalid_1's auc: 0.851308\n",
      "[496]\ttraining's auc: 0.906615\tvalid_1's auc: 0.85122\n",
      "[497]\ttraining's auc: 0.906652\tvalid_1's auc: 0.851337\n",
      "[498]\ttraining's auc: 0.90685\tvalid_1's auc: 0.850928\n",
      "[499]\ttraining's auc: 0.906843\tvalid_1's auc: 0.850928\n",
      "[500]\ttraining's auc: 0.906937\tvalid_1's auc: 0.850782\n",
      "Model successfully saved at: Output\\light_AllMetrics.model\n"
     ]
    }
   ],
   "source": [
    "# Train the final model with the best parameters\n",
    "light_model = LGBMClassifier(**best_params)\n",
    "light_model.fit(train_X, train_y, eval_set=[(train_X, train_y), (valid_X, valid_y)])\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(light_model, model_path)\n",
    "print(\"Model successfully saved at:\", model_path)\n",
    "\n",
    "# Predict and evaluate the model\n",
    "y_pred1 = light_model.predict_proba(valid_X)[:, 1]\n",
    "auc1 = roc_auc_score(valid_y, y_pred1)\n",
    "y_pred1 = (y_pred1 >= 0.5) * 1\n",
    "a = confusion_matrix(valid_y, y_pred1).tolist()\n",
    "a0 = str(a[0])\n",
    "a1 = str(a[1])\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "Kfold_auc = 1 - trials.best_trial['result']['loss']\n",
    "K_AUC = str('K_AUC: %.4f' % Kfold_auc)\n",
    "Precesion = str('Precision: %.4f' % metrics.precision_score(valid_y, y_pred1))\n",
    "Recall = str('Recall: %.4f' % metrics.recall_score(valid_y, y_pred1))\n",
    "F1_score = str('F1-score: %.4f' % metrics.f1_score(valid_y, y_pred1))\n",
    "Accuracy = str('Accuracy: %.4f' % metrics.accuracy_score(valid_y, y_pred1))\n",
    "AUC = str('AUC: %.4f' % auc1)\n",
    "AP = str('AP: %.4f' % metrics.average_precision_score(valid_y, y_pred1))\n",
    "Log_loss = str('Log_loss: %.4f' % metrics.log_loss(valid_y, y_pred1, eps=1e-15, normalize=True, sample_weight=None, labels=None))\n",
    "kappa_score = str('Kappa_score: %.4f' % metrics.cohen_kappa_score(valid_y, y_pred1))\n",
    "confusion_matrix = f'{a0}\\n{a1}\\n'\n",
    "metrics = f'{K_AUC}\\n{AUC}\\n{Precesion}\\n{Recall}\\n{F1_score}\\n{Accuracy}\\n{AP}\\n{Log_loss}\\n{kappa_score}\\n'\n",
    "\n",
    "# Process feature importance\n",
    "my_dict = dict(zip(train_X.columns, light_model.feature_importances_))\n",
    "sorted_dict = dict(sorted(my_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "total = sum(sorted_dict.values())\n",
    "dict1 = {key: (value / total) * 100 for key, value in sorted_dict.items()}\n",
    "\n",
    "# Save evaluation metrics and feature importance to a file\n",
    "with open(filename, 'w') as f:\n",
    "    f.write('---------Confusion Matrix---------\\n')\n",
    "    f.write(confusion_matrix)\n",
    "    f.write('--------Evaluation Metrics--------\\n')\n",
    "    f.write(metrics)\n",
    "    f.write('-------Feature Importance---------\\n')\n",
    "    for key, value in dict1.items():\n",
    "        f.write(f'{key}: {value:.2f}\\n')\n",
    "    f.write('----------Best Parameters---------\\n')\n",
    "    f.write(str(best_params))\n",
    "    f.write('\\n')\n",
    "    seed_str = f'seed = {seed}'\n",
    "    f.write('----------Seed Value--------------\\n')\n",
    "    f.write(seed_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
