{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc64b1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required Python libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "from lightgbm import LGBMClassifier\n",
    "import joblib\n",
    "\n",
    "# Specify the type of features to use: choose either all available metrics or derived indicators only\n",
    "DEMorAll = \"AllMetrics\"  # Options: \"DemDerivedIndicators\" OR \"AllMetrics\"\n",
    "\n",
    "# Load the dataset containing feature and target data\n",
    "data01 = pd.read_csv('1844Points.csv', encoding='gbk')  # Load data with GBK encoding\n",
    "\n",
    "# Rename columns for better clarity and understanding\n",
    "data01.columns = ['FID', 'X', 'Y', 'Aspect', 'Elevation', 'Distance to lineament', 'Lineament density', \n",
    "                  'NDVI', 'Plan curvature', 'Profile curvature', 'Slope', 'Slope length', 'STI',\n",
    "                  'SPI', 'TPI', 'TWI', 'VRM', 'LULC', 'Habitat', 'GDP',\n",
    "                  'Distance from River', 'Distance from Road', 'target']\n",
    "\n",
    "# Drop unnecessary columns based on the feature selection type\n",
    "columns_to_drop = ['FID', 'X', 'Y'] if DEMorAll == 'AllMetrics' else ['FID', 'X', 'Y', \n",
    "                                                                     'LULC', 'Habitat', 'GDP', 'NDVI', 'Distance from Road']\n",
    "data01.drop(columns=columns_to_drop, inplace=True)  # Remove the specified columns from the dataset\n",
    "\n",
    "# Define the path to save or load the LightGBM model\n",
    "model_path = 'Output\\light_{}.model'.format(DEMorAll)\n",
    "\n",
    "# Set a random seed for reproducibility of results\n",
    "seed = 4\n",
    "\n",
    "# Specify filenames for importing and exporting additional datasets\n",
    "filename_import = 'TheLocationOfTheStudyArea.csv'  # Input CSV file containing study area data\n",
    "filename_export = 'Output\\TheLocation_light_{}.csv'.format(DEMorAll)  # Output file for processed data\n",
    "\n",
    "# Load the dataset for the study area's locations and features\n",
    "data_all = pd.read_csv(filename_import)\n",
    "\n",
    "# Rename the columns of the loaded dataset for consistency\n",
    "data_all.columns = ['FID', 'X', 'Y', 'Aspect', 'Elevation', 'Distance to lineament', 'Lineament density', \n",
    "                    'NDVI', 'Plan curvature', 'Profile curvature', 'Slope', 'Slope length', 'STI',\n",
    "                    'SPI', 'TPI', 'TWI', 'VRM', 'LULC', 'Habitat', 'GDP',\n",
    "                    'Distance from River', 'Distance from Road']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56520af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset for predictions\n",
    "# Drop specified columns from the dataset to prepare the feature matrix\n",
    "X_all = data_all.drop(columns=columns_to_drop)\n",
    "seed = 4  # Set random seed for reproducibility\n",
    "\n",
    "# Save the column names for later use\n",
    "header = data01.columns.tolist()\n",
    "\n",
    "# Split the dataset into two dataframes based on the target value\n",
    "data_0 = data01.loc[data01['target'] == 0]  # Data where target = 0\n",
    "data_1 = data01.loc[data01['target'] == 1]  # Data where target = 1\n",
    "\n",
    "# Split data where target = 0 into 80% training and 20% validation sets\n",
    "data_0_X = data_0.drop(columns=[\"target\"], axis=1)  # Features\n",
    "data_0_Y = data_0.target  # Target variable\n",
    "train_0_X, valid_0_X, train_0_y, valid_0_y = train_test_split(data_0_X, data_0_Y, test_size=0.2, random_state=seed)\n",
    "# Combine features and target back into DataFrames for saving\n",
    "save_TrainDate_0 = pd.DataFrame(np.column_stack([train_0_X, train_0_y]), columns=header)\n",
    "save_ValidDate_0 = pd.DataFrame(np.column_stack([valid_0_X, valid_0_y]), columns=header)\n",
    "\n",
    "# Split data where target = 1 into 80% training and 20% validation sets\n",
    "data_1_X = data_1.drop(columns=[\"target\"], axis=1)  # Features\n",
    "data_1_Y = data_1.target  # Target variable\n",
    "train_1_X, valid_1_X, train_1_y, valid_1_y = train_test_split(data_1_X, data_1_Y, test_size=0.2, random_state=seed)\n",
    "# Combine features and target back into DataFrames for saving\n",
    "save_TrainDate_1 = pd.DataFrame(np.column_stack([train_1_X, train_1_y]), columns=header)\n",
    "save_ValidDate_1 = pd.DataFrame(np.column_stack([valid_1_X, valid_1_y]), columns=header)\n",
    "\n",
    "# Combine the training datasets and shuffle the data to avoid any ordering bias\n",
    "train_date = pd.concat([save_TrainDate_0, save_TrainDate_1])\n",
    "train_date = train_date.sample(frac=1, random_state=42)\n",
    "\n",
    "# Combine the validation datasets and shuffle the data to avoid any ordering bias\n",
    "valid_date = pd.concat([save_ValidDate_0, save_ValidDate_1])\n",
    "valid_date = valid_date.sample(frac=1, random_state=42)\n",
    "\n",
    "# Separate features (X) and target (y) from the training dataset\n",
    "train_y = train_date.target  # Training target\n",
    "train_X = train_date.drop(columns=[\"target\"], axis=1)  # Training features\n",
    "\n",
    "# Separate features (X) and target (y) from the validation dataset\n",
    "valid_y = valid_date.target  # Validation target\n",
    "valid_X = valid_date.drop(columns=[\"target\"], axis=1)  # Validation features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b1bc2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* Best Score *******\n",
      "0.850781592403214\n",
      "DOWN\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the pre-trained LightGBM model\n",
    "light_model = joblib.load(model_path)\n",
    "\n",
    "# Retrieve and print the best AUC score from the model\n",
    "best_score = light_model.best_score_['valid_1']['auc']\n",
    "print(\"******* Best Score *******\")\n",
    "print(best_score)\n",
    "\n",
    "# Process raster-to-point data, preparing for prediction\n",
    "# Save column names from the feature matrix\n",
    "header = X_all.columns.tolist()\n",
    "\n",
    "# Extract latitude and longitude from the dataset by dropping feature columns\n",
    "latitude_and_longitude = data_all.drop(columns=header, axis=1)\n",
    "\n",
    "# Align columns in X_all to match the order of train_X for consistency in predictions\n",
    "X_all = X_all.reindex(columns=train_X.columns)\n",
    "\n",
    "# Perform predictions using the pre-trained model\n",
    "Y_all = light_model.predict_proba(X_all)[:, 1]  # Predict probabilities for the positive class\n",
    "# Uncomment below if you want binary predictions instead of probabilities\n",
    "# Y_all = (Y_all >= 0.5) * 1\n",
    "\n",
    "# Convert predictions to a DataFrame for easy export\n",
    "Y_all = pd.DataFrame(Y_all, columns=['target'])\n",
    "\n",
    "# Merge the predictions with the latitude and longitude\n",
    "merge_XY = pd.concat([latitude_and_longitude, X_all, Y_all], axis=1)\n",
    "\n",
    "# Extract only latitude, longitude, and target columns for final output\n",
    "merge_XY = pd.concat([merge_XY.X, merge_XY.Y, merge_XY.target], axis=1)\n",
    "\n",
    "# Save the final output to a CSV file\n",
    "merge_XY.to_csv(filename_export, index=False)\n",
    "\n",
    "print('DOWN')  # Indicate completion of the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbed3f12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
